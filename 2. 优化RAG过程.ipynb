{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 持续优化 LlamaIndex\n",
    "\n",
    "### 1 从提示词与模型角度入手\n",
    "\n",
    "- 通过提示词让大模型反思、总结，补充用户对话的信息不足\n",
    "- 通过大模型与embeddings模型的选择，让大模型更符合业务需求\n",
    "\n",
    "### 2 从业务角度入手\n",
    "\n",
    "- 文本分割方法调整\n",
    "- 如何处理新增内容\n",
    "- 如何更改向量数据库\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 理论知识：怎么定义分段(chunk)，怎么直接查看分段\n",
    "\n",
    "##### 文本分块参数说明\n",
    "\n",
    "大模型的单位 Token，100个 Token 约等于 75个汉字\n",
    "\n",
    "##### chunk_size（分块大小）\n",
    "- 定义了每个文本块的最大长度（通常以字符或token为单位）\n",
    "- 较大的chunk_size意味着：\n",
    "  - 每个块包含更多上下文信息\n",
    "  - 可能会提高答案的准确性\n",
    "  - 但会增加处理时间和内存使用\n",
    "- 较小的chunk_size意味着：\n",
    "  - 处理更快，内存使用更少\n",
    "  - 但可能会丢失一些上下文信息\n",
    "\n",
    "##### chunk_overlap（分块重叠）\n",
    "- 定义了相邻文本块之间重叠的部分大小\n",
    "- 重叠的目的是：\n",
    "  - 保持文本块之间的连贯性\n",
    "  - 避免在分块边界处丢失重要信息\n",
    "  - 确保跨越多个块的概念能被完整捕获\n",
    "\n",
    "##### 建议值\n",
    "##### 对于中文文本：\n",
    "- **chunk_size**: 通常设置在 200-500 之间\n",
    "- **chunk_overlap**: 通常设置为 chunk_size 的 10%-20%\n",
    "\n",
    "> 注：具体值需要根据您的应用场景和文本特点来调整\n",
    "\n",
    "\n",
    "Web Page Reader\n",
    "\n",
    "https://docs.llamaindex.ai/en/stable/examples/data_connectors/WebPageDemo/#using-trafilaturawebreader\n",
    "\n",
    "%pip install llama-index-readers-web trafilatura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文档数量是 2 个\n",
      "第一个文档的前200个字符：\n",
      "Transformer模型架构，是2017 年，Google 在论文 Attentions is All you need 中提出的模型，其使用 Self-Attention 结构取代了在 NLP 任务中常用的 RNN 网络结构。相比 RNN 网络结构，其最大的优点是可以并行计算。\n",
      "Transformer 本质上是一个 Encoder-Decoder 架构。因此中间部分的 Transformer \n",
      "1133\n",
      "Node解析器：include_metadata=True include_prev_next_rel=True callback_manager=<llama_index.core.callbacks.base.CallbackManager object at 0x000001AE13E0DC90> id_func=<function default_id_func at 0x000001AE10A34A40> chunk_size=40 chunk_overlap=10 separator=' ' paragraph_separator='\\n\\n\\n' secondary_chunking_regex='[^,.;。？！]+[,.;。？！]?'\n",
      "Metadata length (0) is close to chunk size (40). Resulting chunks are less than 50 tokens. Consider increasing the chunk size or decreasing the size of your metadata to avoid this.\n",
      "Metadata length (0) is close to chunk size (40). Resulting chunks are less than 50 tokens. Consider increasing the chunk size or decreasing the size of your metadata to avoid this.\n",
      "分割后的节点数量：24\n",
      "第一个节点的内容：Transformer模型架构，是2017 年，Google 在论文 Attentions is All you need 中提出的模型，其使用 Self-Attention\n",
      "第一个节点的长度：87\n",
      "第二个节点的内容：Self-Attention 结构取代了在 NLP 任务中常用的 RNN 网络结构。\n",
      "第二个节点的长度：42\n",
      "第三个节点的内容：RNN 网络结构。相比 RNN 网络结构，其最大的优点是可以并行计算。\n",
      "第三个节点的长度：35\n",
      "--------------------------------\n",
      "完整的Document：\n",
      "    Doc ID: https://baike.baidu.com/item/Transformer模型架构\n",
      "Text: Transformer模型架构，是2017 年，Google 在论文 Attentions is All you need\n",
      "中提出的模型，其使用 Self-Attention 结构取代了在 NLP 任务中常用的 RNN 网络结构。相比 RNN\n",
      "网络结构，其最大的优点是可以并行计算。 Transformer 本质上是一个 Encoder-Decoder 架构。因此中间部分的\n",
      "Transformer 可以分为两个部分：编码组件和解码组件 [1] - 中文名 - Transformer模型架构 - 提出者 2017\n",
      "年，Google 在论文 Attentions is All you need 中提出了Transformer模型架构。 [1]\n",
      "Transformer模型架构使用 Self-A...\n",
      "完整的Node1：\n",
      "    Node ID: f377c5d2-1850-450c-8e71-73361fc47839\n",
      "Text: Transformer模型架构，是2017 年，Google 在论文 Attentions is All you need\n",
      "中提出的模型，其使用 Self-Attention\n",
      "完整的Node2：\n",
      "    Node ID: 0425d961-6c07-45da-b32c-233030698d92\n",
      "Text: Self-Attention 结构取代了在 NLP 任务中常用的 RNN 网络结构。\n",
      "完整的Node3：\n",
      "    Node ID: e53c0ff2-c4e0-4f5f-8d08-7db241175128\n",
      "Text: RNN 网络结构。相比 RNN 网络结构，其最大的优点是可以并行计算。\n",
      "--------------------------------\n",
      "Node关系1：\n",
      "    {<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='https://baike.baidu.com/item/Transformer模型架构', node_type=<ObjectType.DOCUMENT: '4'>, metadata={}, hash='115a825b91c5bf8b8b2cc1a7234ed7dd5b670ded9c6b9cb6f565146fff536bbf'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='0425d961-6c07-45da-b32c-233030698d92', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='ffc86113f83e6c6e68ec2a7ba7dd7455de2970c2d6ff76711635c9aefd6d0908')}\n",
      "Node关系2：\n",
      "    {<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='https://baike.baidu.com/item/Transformer模型架构', node_type=<ObjectType.DOCUMENT: '4'>, metadata={}, hash='115a825b91c5bf8b8b2cc1a7234ed7dd5b670ded9c6b9cb6f565146fff536bbf'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='f377c5d2-1850-450c-8e71-73361fc47839', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='7dc05297d69fd68452700bb67c5e07ed986cd8a850c9d94ca5be8c56c75e1286'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='e53c0ff2-c4e0-4f5f-8d08-7db241175128', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='e35c60c31d190ce5af7a0720e4ea0201a9ea5c7141e784a644d8861aff10976d')}\n",
      "Node关系3：\n",
      "    {<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='https://baike.baidu.com/item/Transformer模型架构', node_type=<ObjectType.DOCUMENT: '4'>, metadata={}, hash='115a825b91c5bf8b8b2cc1a7234ed7dd5b670ded9c6b9cb6f565146fff536bbf'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='0425d961-6c07-45da-b32c-233030698d92', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='ffc86113f83e6c6e68ec2a7ba7dd7455de2970c2d6ff76711635c9aefd6d0908'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='cf2a5913-9570-48e1-a4be-4d74ae5ec71d', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='824cc8453dc93c279b922b3b3ac10e54b21340ce5939d87aebe1f0ba891a940d')}\n"
     ]
    }
   ],
   "source": [
    "from llama_index.readers.web import TrafilaturaWebReader\n",
    " \n",
    "documents = TrafilaturaWebReader().load_data(\n",
    "         [\"https://baike.baidu.com/item/Transformer模型架构\",\n",
    "          \"https://baike.baidu.com/item/LangChain\"]\n",
    ")\n",
    "\n",
    "print(f\"文档数量是 {len(documents)} 个\")\n",
    "print(\"第一个文档的前200个字符：\")\n",
    "print(documents[0].text[:200])\n",
    "print(len(documents[0].text))       \n",
    "\n",
    "\n",
    "\n",
    "from llama_index.core.node_parser import SimpleNodeParser\n",
    "\n",
    "\n",
    "# 解析器(parser)用于分割文档(document)为节点(node)\n",
    "node_parser = SimpleNodeParser().from_defaults(\n",
    "    chunk_size=40,\n",
    "    chunk_overlap=10\n",
    ")\n",
    "\n",
    "print(f\"Node解析器：{node_parser}\")\n",
    "\n",
    "# 分割文档\n",
    "nodes = node_parser.get_nodes_from_documents(documents)\n",
    "\n",
    "print(f\"分割后的节点数量：{len(nodes)}\")\n",
    "print(f\"第一个节点的内容：{nodes[0].text}\")\n",
    "print(f\"第一个节点的长度：{len(nodes[0].text)}\")\n",
    "print(f\"第二个节点的内容：{nodes[1].text}\")\n",
    "print(f\"第二个节点的长度：{len(nodes[1].text)}\")\n",
    "print(f\"第三个节点的内容：{nodes[2].text}\")\n",
    "print(f\"第三个节点的长度：{len(nodes[2].text)}\")\n",
    "\n",
    "print(\"--------------------------------\")\n",
    "print(f\"完整的Document：\\n    {documents[0]}\")\n",
    "print(f\"完整的Node1：\\n    {nodes[0]}\")\n",
    "print(f\"完整的Node2：\\n    {nodes[1]}\")\n",
    "print(f\"完整的Node3：\\n    {nodes[2]}\")\n",
    "print(\"--------------------------------\")\n",
    "print(f\"Node关系1：\\n    {nodes[0].relationships}\")\n",
    "print(f\"Node关系2：\\n    {nodes[1].relationships}\")\n",
    "print(f\"Node关系3：\\n    {nodes[2].relationships}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 文本分割方法调整-- JSON 解析器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_unstructured_db(db_name:str,label_name:list):\n",
    "    print(f\"知识库名称为：{db_name}，类目名称为：{label_name}\")\n",
    "    if label_name is None:\n",
    "        gr.Info(\"没有选择类目\")\n",
    "    elif len(db_name) == 0:\n",
    "        gr.Info(\"没有命名知识库\")\n",
    "    # 判断是否存在同名向量数据库\n",
    "    elif db_name in os.listdir(DB_PATH):\n",
    "        gr.Info(\"知识库已存在，请换个名字或删除原来知识库再创建\")\n",
    "    else:\n",
    "        gr.Info(\"正在创建知识库，请等待知识库创建成功信息显示后前往RAG问答\")\n",
    "        nodes = []\n",
    "        for label in label_name:\n",
    "            label_path = os.path.join(UNSTRUCTURED_FILE_PATH,label)\n",
    "            nodes.extend(SimpleDirectoryReader(label_path).load_data())\n",
    "\n",
    "        # 添加json解析和句子窗口解析\n",
    "        from llama_index.core.node_parser import JSONNodeParser, SentenceWindowNodeParser\n",
    "        \n",
    "        # 先用JSON解析器处理\n",
    "        json_parser = JSONNodeParser()\n",
    "        nodes = json_parser.get_nodes_from_documents(nodes, show_progress=True)\n",
    "        \n",
    "        # 创建索引并保存\n",
    "        index = VectorStoreIndex(nodes)\n",
    "        db_path = os.path.join(DB_PATH, db_name)\n",
    "        if not os.path.exists(db_path):\n",
    "            os.mkdir(db_path)\n",
    "        index.storage_context.persist(db_path)\n",
    "        gr.Info(\"知识库创建成功，可前往RAG问答进行提问\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 2.1 文本分割方法调整-- 句子窗口解析器\n",
    "[ Llamaindex 官方文档 ：基本 RAG 流程](https://docs.llamaindex.ai/en/stable/understanding/rag/)\n",
    "\n",
    "\n",
    "## 基本 RAG 的工作流程与局限性\n",
    "\n",
    "### 基本 RAG 流程\n",
    "1. **文档分块**：按指定的 chunk size 将文档切分\n",
    "2. **向量化处理**：对文档块进行 embedding 向量化\n",
    "3. **相似度检索**：计算用户问题与文档块的相似度，选取 Top-K 相关文档作为 context\n",
    "4. **LLM 生成**：将 context 和用户问题一起发送给 LLM 生成回答\n",
    "\n",
    "### 核心问题：chunk size 的两难困境\n",
    "\n",
    "### 小 chunk size：\n",
    "- ✅ 与问题的匹配度高\n",
    "- ❌ context 信息量不足\n",
    "- ❌ 可能导致回答质量下降\n",
    "\n",
    "### 大 chunk size：\n",
    "- ✅ context 信息量充足\n",
    "- ❌ 与问题的匹配精度下降\n",
    "- ❌ 同样可能影响回答质量\n",
    "\n",
    "### 句子窗口解析器(Sentence Window Parser)的优势\n",
    "\n",
    "### 核心思想\n",
    "在保持高匹配精度的同时，通过上下文窗口提供更多相关信息\n",
    "\n",
    "### 工作方式\n",
    "1. 先将文档切分为较小的文档块\n",
    "2. 匹配到相关文档块后，自动获取其周围的文档内容作为补充 context\n",
    "3. 通过 `window_size` 参数控制获取周围内容的范围\n",
    "\n",
    "### 主要参数\n",
    "```python\n",
    "SentenceWindowNodeParser.from_defaults(\n",
    "    window_size=2,                            # 在匹配块两侧各取2个句子\n",
    "    window_metadata_key=\"window\",             # 窗口信息的元数据键\n",
    "    original_text_metadata_key=\"original_text\" # 原始文本的元数据键\n",
    ")\n",
    "```\n",
    "\n",
    "这种方法既保证了检索的精确性，又能提供足够的上下文信息，有效解决了基本 RAG 中 chunk size 的两难问题。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node解析器：include_metadata=True include_prev_next_rel=True callback_manager=<llama_index.core.callbacks.base.CallbackManager object at 0x000001AE16D2C810> id_func=<function default_id_func at 0x000001AE10A34A40> sentence_splitter=<function split_by_sentence_tokenizer.<locals>.<lambda> at 0x000001AE166C1440> window_size=3 window_metadata_key='window' original_text_metadata_key='original_text'\n",
      "--------------------------------\n",
      "['公安机关调取了一份行政机关收集的视听资料作为证据，但该视听资料已经被改动过。这份视听资料是否可以作为证据使用呢？']\n",
      "窗口数据和文档数据\n",
      "{'window': '公安机关调取了一份行政机关收集的视听资料作为证据，但该视听资料已经被改动过。这份视听资料是否可以作为证据使用呢？', 'original_text': '公安机关调取了一份行政机关收集的视听资料作为证据，但该视听资料已经被改动过。这份视听资料是否可以作为证据使用呢？'}\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.node_parser import SentenceWindowNodeParser\n",
    " \n",
    "#定义句子窗口 Node 解析器\n",
    "node_parser = SentenceWindowNodeParser.from_defaults(\n",
    "    window_size=3,\n",
    "    window_metadata_key=\"window\",\n",
    "    original_text_metadata_key=\"original_text\",\n",
    ")\n",
    " \n",
    "print(f\"Node解析器：{node_parser}\")\n",
    "print(\"--------------------------------\")\n",
    "\n",
    "from llama_index.core import Document\n",
    " \n",
    "text = \"公安机关调取了一份行政机关收集的视听资料作为证据，但该视听资料已经被改动过。这份视听资料是否可以作为证据使用呢？\"\n",
    "\n",
    "\n",
    "#  处理中文标点，否则不会分割\n",
    "# text = text.replace(\"。\", \". \") \\\n",
    "#          .replace(\"？\", \"? \") \\\n",
    "#          .replace(\"！\", \"! \") \\\n",
    "#          .replace(\"；\", \"; \") \\\n",
    "#          .replace(\"：\", \": \") \\\n",
    "#          .replace(\"，\", \", \") \\\n",
    "#          .replace(\"、\", \", \")\n",
    "\n",
    "nodes = node_parser.get_nodes_from_documents([Document(text=text)])\n",
    "\n",
    "print([x.text for x in nodes])\n",
    "print(\"窗口数据和文档数据\")\n",
    "print(nodes[0].metadata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建非结构化向量数据库\n",
    "def create_unstructured_db(db_name:str,label_name:list):\n",
    "    print(f\"知识库名称为：{db_name}，类目名称为：{label_name}\")\n",
    "    if label_name is None:\n",
    "        gr.Info(\"没有选择类目\")\n",
    "    elif len(db_name) == 0:\n",
    "        gr.Info(\"没有命名知识库\")\n",
    "    # 判断是否存在同名向量数据库\n",
    "    elif db_name in os.listdir(DB_PATH):\n",
    "        gr.Info(\"知识库已存在，请换个名字或删除原来知识库再创建\")\n",
    "    else:\n",
    "        gr.Info(\"正在创建知识库，请等待知识库创建成功信息显示后前往RAG问答\")\n",
    "        nodes = []\n",
    "        for label in label_name:\n",
    "            label_path = os.path.join(UNSTRUCTURED_FILE_PATH,label)\n",
    "            nodes.extend(SimpleDirectoryReader(label_path).load_data())\n",
    "\n",
    "        # 添加json解析和句子窗口解析\n",
    "        from llama_index.core.node_parser import JSONNodeParser, SentenceWindowNodeParser\n",
    "        \n",
    "        # 创建一个新的列表存储解析后的节点\n",
    "        parsed_nodes = []\n",
    "        \n",
    "        # 根据文档类型选择合适的解析器\n",
    "        for doc in nodes:\n",
    "            if doc.metadata.get(\"file_type\") == \"json\":\n",
    "                # 如果是JSON文档，使用JSON解析器\n",
    "                json_parser = JSONNodeParser()\n",
    "                doc_parsed_nodes = json_parser.get_nodes_from_documents([doc], show_progress=True)\n",
    "            else:\n",
    "                # 如果是普通文本，直接使用句子窗口解析器\n",
    "                sentence_parser = SentenceWindowNodeParser.from_defaults(\n",
    "                    window_size=2,\n",
    "                    window_metadata_key=\"window\",\n",
    "                    original_text_metadata_key=\"original_text\"\n",
    "                )\n",
    "                doc_parsed_nodes = sentence_parser.get_nodes_from_documents([doc], show_progress=True)\n",
    "            parsed_nodes.extend(doc_parsed_nodes)\n",
    "        \n",
    "        # 将解析后的节点添加到原始节点列表\n",
    "        nodes.extend(parsed_nodes)\n",
    "    \n",
    "        # 创建索引并保存\n",
    "        index = VectorStoreIndex(nodes)\n",
    "        db_path = os.path.join(DB_PATH, db_name)\n",
    "        if not os.path.exists(db_path):\n",
    "            os.mkdir(db_path)\n",
    "        index.storage_context.persist(db_path)\n",
    "        gr.Info(\"知识库创建成功，可前往RAG问答进行提问\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 文本分割方法调整-- 组合解析器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import ComposableNodeParser\n",
    "\n",
    "# 创建组合解析器\n",
    "composable_parser = ComposableNodeParser.from_defaults(\n",
    "    nodes_parsers=[\n",
    "        JSONNodeParser(),\n",
    "        SentenceWindowNodeParser.from_defaults(\n",
    "            window_size=3,\n",
    "            window_metadata_key=\"window\",\n",
    "            original_text_metadata_key=\"original_text\"\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 使用组合解析器\n",
    "nodes = composable_parser.get_nodes_from_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 如何更新 Index 处理新增内容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import StorageContext, load_index_from_storage\n",
    "def add_to_existing_db(db_name: str, new_files: list):\n",
    "    \"\"\"向现有知识库添加新文档，无需重建整个索引\"\"\"\n",
    "    \n",
    "    # 核心原理1: 检查并加载现有索引\n",
    "    # - 向量数据库的持久化存储包含了索引结构和向量数据\n",
    "    # - StorageContext 可以从持久化目录中恢复完整的索引状态\n",
    "    if db_name not in os.listdir(DB_PATH):\n",
    "        gr.Info(\"指定的知识库不存在\")\n",
    "        return\n",
    "    \n",
    "    db_path = os.path.join(DB_PATH, db_name)\n",
    "    storage_context = StorageContext.from_defaults(persist_dir=db_path)\n",
    "    index = load_index_from_storage(storage_context)\n",
    "    \n",
    "    # 核心原理2: 新文档的处理流程\n",
    "    # - 使用 SimpleDirectoryReader 读取新文件\n",
    "    # - 保持与原始索引创建时相同的文档处理方式\n",
    "    new_documents = SimpleDirectoryReader(input_files=new_files).load_data()\n",
    "    new_nodes = []\n",
    "    \n",
    "    # 核心原理3: 文档解析策略\n",
    "    # - 保持与原始索引相同的解析方式，确保新旧节点的一致性\n",
    "    # - JSON文档和普通文本使用不同的解析器\n",
    "    for doc in new_documents:\n",
    "        if doc.metadata.get(\"file_type\") == \"json\":\n",
    "            json_parser = JSONNodeParser()\n",
    "            doc_parsed_nodes = json_parser.get_nodes_from_documents([doc], show_progress=True)\n",
    "        else:\n",
    "            # 使用句子窗口解析器处理普通文本\n",
    "            # - window_size=2 表示在匹配句子前后各保留2个句子作为上下文\n",
    "            sentence_parser = SentenceWindowNodeParser.from_defaults(\n",
    "                window_size=2,\n",
    "                window_metadata_key=\"window\",\n",
    "                original_text_metadata_key=\"original_text\"\n",
    "            )\n",
    "            doc_parsed_nodes = sentence_parser.get_nodes_from_documents([doc], show_progress=True)\n",
    "        new_nodes.extend(doc_parsed_nodes)\n",
    "    \n",
    "    # 核心原理4: 增量更新机制\n",
    "    # - insert_nodes 方法实现了向量数据库的增量更新\n",
    "    # - 只计算和存储新节点的向量，不影响现有的向量数据\n",
    "    # - 新节点会被添加到现有的向量空间中，保持索引结构的完整性\n",
    "    index.insert_nodes(new_nodes)\n",
    "    \n",
    "    # 核心原理5: 持久化更新\n",
    "    # - persist 方法将更新后的索引状态保存到磁盘\n",
    "    # - 只保存增量更新的部分，不会重写整个索引文件\n",
    "    index.storage_context.persist(db_path)\n",
    "    gr.Info(\"新文档已成功添加到知识库中\")\n",
    "\n",
    "# 调用方法： add_to_existing_db(\"知识库名称\", [\"新文件路径1\", \"新文件路径2\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "添加了新的 add_to_existing_db 函数，接受知识库名称和新文件列表作为参数\n",
    "函数会：\n",
    "检查知识库是否存在\n",
    "加载现有的索引\n",
    "处理新文件并创建节点\n",
    "将新节点添加到现有索引中\n",
    "保存更新后的索引"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 更改向量数据库\n",
    "在LlamaIndex中，默认使用的是SimpleVectorStore作为向量数据库，它是一个基于内存的简单向量存储实现。我们可以轻松地更改为其他向量数据库。以下是一些常用的替代方案：\n",
    "\n",
    "1. ChromaDB：ChromaDB是一个开源的向量数据库，支持高效的向量搜索和存储。\n",
    "2. Zilliz：Zilliz是一个分布式向量数据库，支持大规模数据存储和高效的向量搜索。\n",
    "3. Faiss：Faiss是一个高效的向量搜索库，支持快速的向量索引和搜索。\n",
    "4. Milvus：Milvus是一个分布式向量数据库，支持高效的向量搜索和存储。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from chromadb import PersistentClient\n",
    "import chromadb\n",
    "\n",
    "# ... existing code ...\n",
    "\n",
    "def create_unstructured_db(db_name:str,label_name:list):\n",
    "    # ... existing code ...\n",
    "    \n",
    "    # 创建Chroma客户端和集合\n",
    "    chroma_client = PersistentClient(path=os.path.join(DB_PATH, db_name))\n",
    "    chroma_collection = chroma_client.create_collection(name=db_name)\n",
    "    \n",
    "    # 创建向量存储\n",
    "    vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "    storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "    \n",
    "    # 创建索引\n",
    "    index = VectorStoreIndex(nodes, storage_context=storage_context)\n",
    "    index.storage_context.persist()\n",
    "    \n",
    "    gr.Info(\"知识库创建成功，可前往RAG问答进行提问\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用Milvus向量数据库的示例\n",
    "from llama_index.vector_stores.milvus import MilvusVectorStore\n",
    "\n",
    "# ... existing code ...\n",
    "\n",
    "def create_unstructured_db(db_name:str,label_name:list):\n",
    "    # ... existing code ...\n",
    "    \n",
    "    # 创建Milvus向量存储\n",
    "    vector_store = MilvusVectorStore(\n",
    "        collection_name=db_name,\n",
    "        dim=1536,  # 根据您使用的嵌入模型维度设置\n",
    "        host=\"localhost\",\n",
    "        port=19530\n",
    "    )\n",
    "    storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "    \n",
    "    # 创建索引\n",
    "    index = VectorStoreIndex(nodes, storage_context=storage_context)\n",
    "    index.storage_context.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用FAISS向量数据库的示例\n",
    "from llama_index.vector_stores.faiss import FaissVectorStore\n",
    "import faiss\n",
    "\n",
    "# ... existing code ...\n",
    "\n",
    "def create_unstructured_db(db_name:str,label_name:list):\n",
    "    # ... existing code ...\n",
    "    \n",
    "    # 创建FAISS向量存储\n",
    "    dimension = 1536  # 根据您使用的嵌入模型维度设置\n",
    "    faiss_index = faiss.IndexFlatL2(dimension)\n",
    "    vector_store = FaissVectorStore(faiss_index=faiss_index)\n",
    "    storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "    \n",
    "    # 创建索引\n",
    "    index = VectorStoreIndex(nodes, storage_context=storage_context)\n",
    "    index.storage_context.persist(os.path.join(DB_PATH, db_name))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "主要步骤说明：\n",
    "\n",
    "首先需要安装相应的向量数据库包，如：pip install chromadb、pip install pymilvus或pip install faiss-cpu\n",
    "导入相应的向量存储类\n",
    "\n",
    "创建向量存储实例\n",
    "\n",
    "使用StorageContext配置存储上下文\n",
    "\n",
    "在创建索引时使用自定义的存储上下文\n",
    "\n",
    "每种向量数据库都有其特点：\n",
    "\n",
    "Chroma: 轻量级、易于使用，支持持久化存储\n",
    "\n",
    "Milvus: 分布式向量数据库，适合大规模生产环境\n",
    "\n",
    "FAISS: Facebook开发的高性能向量检索库，适合大规模相似性搜索\n",
    "\n",
    "选择哪种向量数据库主要取决于您的具体需求，如数据规模、查询性能要求、部署环境等。\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local_rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
